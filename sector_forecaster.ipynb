{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sector Forecaster (Colab Ready)\n",
        "\n",
        "This notebook mirrors the functionality of `sector_forecaster.py` while keeping the workflow modular for interactive exploration in environments such as Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install dependencies\n",
        "\n",
        "Run the following cell on Colab to ensure the required libraries are installed. Skip it if you already have them in your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip install --quiet pandas yfinance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and configuration\n",
        "\n",
        "Define the ticker universes, sentiment dictionaries, and helper utilities shared with the standalone script.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Mapping, MutableMapping, Optional, Sequence, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Leader:\n",
        "    ticker: str\n",
        "    name: str\n",
        "    aliases: Tuple[str, ...] = ()\n",
        "\n",
        "    def keywords(self) -> Tuple[str, ...]:\n",
        "        base_keywords = [self.ticker.lower(), self.name.lower()]\n",
        "        base_keywords.extend(alias.lower() for alias in self.aliases)\n",
        "        return tuple(base_keywords)\n",
        "\n",
        "\n",
        "DEFAULT_LEADERS: Mapping[str, Tuple[Leader, ...]] = {\n",
        "    \"Information Technology\": (\n",
        "        Leader(\"AAPL\", \"Apple\", aliases=(\"iphone\", \"macbook\")),\n",
        "        Leader(\"MSFT\", \"Microsoft\", aliases=(\"azure\", \"windows\")),\n",
        "        Leader(\"NVDA\", \"Nvidia\", aliases=(\"geforce\", \"cuda\")),\n",
        "    ),\n",
        "    \"Communication Services\": (\n",
        "        Leader(\"GOOGL\", \"Alphabet\", aliases=(\"google\", \"youtube\")),\n",
        "        Leader(\"META\", \"Meta Platforms\", aliases=(\"facebook\", \"instagram\")),\n",
        "        Leader(\"NFLX\", \"Netflix\"),\n",
        "    ),\n",
        "    \"Consumer Discretionary\": (\n",
        "        Leader(\"AMZN\", \"Amazon\", aliases=(\"aws\", \"prime\")),\n",
        "        Leader(\"TSLA\", \"Tesla\"),\n",
        "        Leader(\"HD\", \"Home Depot\"),\n",
        "    ),\n",
        "    \"Financials\": (\n",
        "        Leader(\"JPM\", \"JPMorgan Chase\", aliases=(\"j.p. morgan\", \"jp morgan\")),\n",
        "        Leader(\"BAC\", \"Bank of America\"),\n",
        "        Leader(\"V\", \"Visa\"),\n",
        "    ),\n",
        "    \"Health Care\": (\n",
        "        Leader(\"UNH\", \"UnitedHealth\", aliases=(\"optum\",)),\n",
        "        Leader(\"JNJ\", \"Johnson & Johnson\", aliases=(\"janssen\",)),\n",
        "        Leader(\"PFE\", \"Pfizer\"),\n",
        "    ),\n",
        "    \"Industrials\": (\n",
        "        Leader(\"CAT\", \"Caterpillar\"),\n",
        "        Leader(\"HON\", \"Honeywell\"),\n",
        "        Leader(\"BA\", \"Boeing\"),\n",
        "    ),\n",
        "    \"Energy\": (\n",
        "        Leader(\"XOM\", \"Exxon Mobil\", aliases=(\"exxon\", \"mobil\")),\n",
        "        Leader(\"CVX\", \"Chevron\"),\n",
        "        Leader(\"SLB\", \"Schlumberger\", aliases=(\"slb\",)),\n",
        "    ),\n",
        "    \"Consumer Staples\": (\n",
        "        Leader(\"PG\", \"Procter & Gamble\", aliases=(\"p&g\", \"tide\")),\n",
        "        Leader(\"KO\", \"Coca-Cola\", aliases=(\"coke\",)),\n",
        "        Leader(\"PEP\", \"PepsiCo\", aliases=(\"pepsi\",)),\n",
        "    ),\n",
        "    \"Utilities\": (\n",
        "        Leader(\"NEE\", \"NextEra Energy\"),\n",
        "        Leader(\"DUK\", \"Duke Energy\"),\n",
        "        Leader(\"SO\", \"Southern Company\", aliases=(\"southern co\",)),\n",
        "    ),\n",
        "    \"Real Estate\": (\n",
        "        Leader(\"PLD\", \"Prologis\"),\n",
        "        Leader(\"AMT\", \"American Tower\"),\n",
        "        Leader(\"EQIX\", \"Equinix\"),\n",
        "    ),\n",
        "    \"Materials\": (\n",
        "        Leader(\"LIN\", \"Linde\"),\n",
        "        Leader(\"SHW\", \"Sherwin-Williams\", aliases=(\"sherwin williams\",)),\n",
        "        Leader(\"NEM\", \"Newmont\"),\n",
        "    ),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "POSITIVE_WORDS: Tuple[str, ...] = (\n",
        "    \"growth\",\n",
        "    \"gain\",\n",
        "    \"gains\",\n",
        "    \"improve\",\n",
        "    \"improved\",\n",
        "    \"improving\",\n",
        "    \"surge\",\n",
        "    \"surged\",\n",
        "    \"surging\",\n",
        "    \"strong\",\n",
        "    \"bullish\",\n",
        "    \"optimistic\",\n",
        "    \"upbeat\",\n",
        "    \"record\",\n",
        "    \"beat\",\n",
        "    \"beats\",\n",
        "    \"beating\",\n",
        "    \"exceed\",\n",
        "    \"exceeds\",\n",
        "    \"expansion\",\n",
        "    \"expand\",\n",
        "    \"expands\",\n",
        "    \"expanding\",\n",
        "    \"profit\",\n",
        "    \"profitable\",\n",
        "    \"profits\",\n",
        "    \"advances\",\n",
        "    \"advance\",\n",
        "    \"advanced\",\n",
        "    \"resilient\",\n",
        "    \"tailwind\",\n",
        "    \"outperform\",\n",
        "    \"outperformance\",\n",
        "    \"lead\",\n",
        "    \"leading\",\n",
        "    \"positive\",\n",
        "    \"constructive\",\n",
        "    \"encourage\",\n",
        "    \"encouraging\",\n",
        "    \"solid\",\n",
        ")\n",
        "\n",
        "NEGATIVE_WORDS: Tuple[str, ...] = (\n",
        "    \"loss\",\n",
        "    \"losses\",\n",
        "    \"decline\",\n",
        "    \"declines\",\n",
        "    \"declining\",\n",
        "    \"drop\",\n",
        "    \"drops\",\n",
        "    \"dropped\",\n",
        "    \"drag\",\n",
        "    \"headwind\",\n",
        "    \"bearish\",\n",
        "    \"weak\",\n",
        "    \"weaker\",\n",
        "    \"weakness\",\n",
        "    \"concern\",\n",
        "    \"concerns\",\n",
        "    \"risk\",\n",
        "    \"risks\",\n",
        "    \"volatile\",\n",
        "    \"volatility\",\n",
        "    \"selloff\",\n",
        "    \"sell-off\",\n",
        "    \"fear\",\n",
        "    \"fears\",\n",
        "    \"slowdown\",\n",
        "    \"slowing\",\n",
        "    \"recession\",\n",
        "    \"warning\",\n",
        "    \"warns\",\n",
        "    \"warned\",\n",
        "    \"pressure\",\n",
        "    \"pressures\",\n",
        "    \"problem\",\n",
        "    \"problems\",\n",
        "    \"challenge\",\n",
        "    \"challenges\",\n",
        "    \"uncertain\",\n",
        "    \"uncertainty\",\n",
        "    \"negative\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def _normalise_range(series: pd.Series) -> pd.Series:\n",
        "    if series.empty:\n",
        "        return series\n",
        "    min_val = series.min()\n",
        "    max_val = series.max()\n",
        "    if math.isclose(min_val, max_val):\n",
        "        return pd.Series([1.0] * len(series), index=series.index)\n",
        "    return (series - min_val) / (max_val - min_val)\n",
        "\n",
        "\n",
        "def _clean_json_payload(raw_text: str) -> str:\n",
        "    stripped = raw_text.strip()\n",
        "    if not stripped:\n",
        "        return \"[]\"\n",
        "    cleaned = re.sub(r\"]\\s*\\[\", \",\", stripped)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def load_news_articles(news_path: Path) -> List[dict]:\n",
        "    text = news_path.read_text(encoding=\"utf-8\")\n",
        "    cleaned = _clean_json_payload(text)\n",
        "    data = json.loads(cleaned)\n",
        "    if not isinstance(data, list):\n",
        "        raise TypeError(f\"Expected a list of articles in {news_path}, got {type(data)!r}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def _tokenise(text: str) -> List[str]:\n",
        "    return re.findall(r\"[A-Za-z']+\", text.lower())\n",
        "\n",
        "\n",
        "def compute_sentiment_score(text: str) -> float:\n",
        "    tokens = _tokenise(text)\n",
        "    if not tokens:\n",
        "        return 0.0\n",
        "    positives = sum(token in POSITIVE_WORDS for token in tokens)\n",
        "    negatives = sum(token in NEGATIVE_WORDS for token in tokens)\n",
        "    return (positives - negatives) / len(tokens)\n",
        "\n",
        "\n",
        "def parse_seendate(value: Optional[str]) -> Optional[datetime]:\n",
        "    if not value:\n",
        "        return None\n",
        "    for fmt in (\"%Y%m%dT%H%M%SZ\", \"%Y%m%dT%H%M%S%z\", \"%Y-%m-%dT%H:%M:%SZ\"):\n",
        "        try:\n",
        "            return datetime.strptime(value, fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SectorForecaster:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sector_leaders: Mapping[str, Sequence[Leader]],\n",
        "        start: str,\n",
        "        end: str,\n",
        "        news_path: Path,\n",
        "    ) -> None:\n",
        "        self.sector_leaders = sector_leaders\n",
        "        self.start = pd.Timestamp(start)\n",
        "        self.end = pd.Timestamp(end)\n",
        "        if self.end <= self.start:\n",
        "            raise ValueError(\"End date must be after start date.\")\n",
        "        self.news_path = news_path\n",
        "        self._history_end = self.end + pd.Timedelta(days=1)\n",
        "\n",
        "    def analyse_news(self) -> pd.DataFrame:\n",
        "        articles = load_news_articles(self.news_path)\n",
        "        if not articles:\n",
        "            return pd.DataFrame(columns=[\"sector\", \"sentiment\", \"article_count\"])\n",
        "\n",
        "        aggregates: MutableMapping[str, Dict[str, float]] = defaultdict(lambda: {\n",
        "            \"weighted_sentiment\": 0.0,\n",
        "            \"weight_sum\": 0.0,\n",
        "            \"article_count\": 0,\n",
        "        })\n",
        "\n",
        "        for article in articles:\n",
        "            title = article.get(\"title\") or \"\"\n",
        "            body = article.get(\"clean_text\") or \"\"\n",
        "            content = f\"{title}\n",
        "{body}\"\n",
        "            sentiment = compute_sentiment_score(content)\n",
        "            article_sectors = self._detect_sectors(content)\n",
        "            if not article_sectors:\n",
        "                continue\n",
        "\n",
        "            published = parse_seendate(article.get(\"seendate\"))\n",
        "            if published is None:\n",
        "                recency_weight = 1.0\n",
        "            else:\n",
        "                distance = (self.end - published).days\n",
        "                recency_weight = math.exp(-max(distance, 0) / 180.0)\n",
        "                recency_weight = max(recency_weight, 0.1)\n",
        "\n",
        "            for sector in article_sectors:\n",
        "                aggregates[sector][\"weighted_sentiment\"] += sentiment * recency_weight\n",
        "                aggregates[sector][\"weight_sum\"] += recency_weight\n",
        "                aggregates[sector][\"article_count\"] += 1\n",
        "\n",
        "        records = []\n",
        "        for sector, stats in aggregates.items():\n",
        "            weight_sum = stats[\"weight_sum\"] or 1.0\n",
        "            records.append(\n",
        "                {\n",
        "                    \"sector\": sector,\n",
        "                    \"sentiment\": stats[\"weighted_sentiment\"] / weight_sum,\n",
        "                    \"article_count\": int(stats[\"article_count\"]),\n",
        "                }\n",
        "            )\n",
        "\n",
        "        sentiment_df = pd.DataFrame(records)\n",
        "        if not sentiment_df.empty:\n",
        "            sentiment_df.sort_values(\"sentiment\", ascending=False, inplace=True)\n",
        "            sentiment_df.reset_index(drop=True, inplace=True)\n",
        "        return sentiment_df\n",
        "\n",
        "    def _detect_sectors(self, content: str) -> List[str]:\n",
        "        content_lower = content.lower()\n",
        "        matched_sectors = []\n",
        "        for sector, leaders in self.sector_leaders.items():\n",
        "            for leader in leaders:\n",
        "                if any(keyword in content_lower for keyword in leader.keywords()):\n",
        "                    matched_sectors.append(sector)\n",
        "                    break\n",
        "        return matched_sectors\n",
        "\n",
        "    def fetch_market_trends(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        sector_frames: List[pd.DataFrame] = []\n",
        "        summary_records: List[dict] = []\n",
        "\n",
        "        for sector, leaders in self.sector_leaders.items():\n",
        "            sector_series: List[pd.Series] = []\n",
        "            start_caps: List[float] = []\n",
        "            end_caps: List[float] = []\n",
        "\n",
        "            for leader in leaders:\n",
        "                ticker = leader.ticker\n",
        "                ticker_obj = yf.Ticker(ticker)\n",
        "                try:\n",
        "                    history = ticker_obj.history(\n",
        "                        start=str(self.start.date()),\n",
        "                        end=str(self._history_end.date()),\n",
        "                        auto_adjust=False,\n",
        "                    )\n",
        "                except Exception as exc:\n",
        "                    print(f\"Warning: failed to download history for {ticker}: {exc}\")\n",
        "                    continue\n",
        "                if history.empty:\n",
        "                    print(f\"Warning: no price history returned for {ticker}.\")\n",
        "                    continue\n",
        "\n",
        "                close_prices = history[\"Close\"].rename(ticker)\n",
        "                shares = self._infer_share_count(ticker_obj, close_prices)\n",
        "                market_caps = close_prices * shares\n",
        "                sector_series.append(market_caps)\n",
        "                start_caps.append(float(market_caps.iloc[0]))\n",
        "                end_caps.append(float(market_caps.iloc[-1]))\n",
        "\n",
        "            if not sector_series:\n",
        "                continue\n",
        "\n",
        "            combined = pd.concat(sector_series, axis=1).sort_index()\n",
        "            combined = combined.ffill().bfill()\n",
        "            combined[\"sector_market_cap\"] = combined.sum(axis=1)\n",
        "            combined[\"sector\"] = sector\n",
        "            sector_frames.append(combined[[\"sector_market_cap\", \"sector\"]])\n",
        "\n",
        "            sector_start = sum(start_caps)\n",
        "            sector_end = sum(end_caps)\n",
        "            growth_rate = (sector_end / sector_start) - 1 if sector_start else 0.0\n",
        "            summary_records.append(\n",
        "                {\n",
        "                    \"sector\": sector,\n",
        "                    \"start_market_cap\": sector_start,\n",
        "                    \"end_market_cap\": sector_end,\n",
        "                    \"growth_rate\": growth_rate,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if not sector_frames:\n",
        "            raise RuntimeError(\"Failed to download market data for all sectors.\")\n",
        "\n",
        "        market_history = pd.concat(sector_frames)\n",
        "        market_history.reset_index(inplace=True)\n",
        "        market_history.rename(columns={\"index\": \"date\"}, inplace=True)\n",
        "        market_history[\"date\"] = pd.to_datetime(market_history[\"date\"])\n",
        "        market_history.sort_values([\"date\", \"sector\"], inplace=True)\n",
        "        market_history.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_records)\n",
        "        summary_df.sort_values(\"growth_rate\", ascending=False, inplace=True)\n",
        "        summary_df.reset_index(drop=True, inplace=True)\n",
        "        return market_history, summary_df\n",
        "\n",
        "    def _infer_share_count(self, ticker_obj: \"yf.Ticker\", close_prices: pd.Series) -> float:\n",
        "        fast_info = getattr(ticker_obj, \"fast_info\", {}) or {}\n",
        "        market_cap = fast_info.get(\"market_cap\")\n",
        "        last_price = fast_info.get(\"last_price\") or fast_info.get(\"previous_close\")\n",
        "        if market_cap and last_price and last_price != 0:\n",
        "            return float(market_cap) / float(last_price)\n",
        "\n",
        "        info = {}\n",
        "        try:\n",
        "            info = ticker_obj.info or {}\n",
        "        except Exception:\n",
        "            info = {}\n",
        "\n",
        "        shares_outstanding = info.get(\"sharesOutstanding\")\n",
        "        if shares_outstanding:\n",
        "            return float(shares_outstanding)\n",
        "\n",
        "        market_cap_info = info.get(\"marketCap\")\n",
        "        if market_cap_info and not close_prices.empty:\n",
        "            return float(market_cap_info) / float(close_prices.iloc[-1])\n",
        "\n",
        "        first_price = float(close_prices.iloc[0]) if not close_prices.empty else 1.0\n",
        "        return 1_000_000_000.0 / max(first_price, 1e-6)\n",
        "\n",
        "    def rank_sectors(self, sentiment_df: pd.DataFrame, growth_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        combined = growth_df.merge(sentiment_df, on=\"sector\", how=\"left\")\n",
        "        combined[\"sentiment\"].fillna(0.0, inplace=True)\n",
        "        combined[\"article_count\"].fillna(0, inplace=True)\n",
        "\n",
        "        combined[\"growth_score\"] = _normalise_range(combined[\"growth_rate\"])\n",
        "        combined[\"sentiment_score\"] = _normalise_range(combined[\"sentiment\"])\n",
        "        combined[\"composite_score\"] = 0.7 * combined[\"growth_score\"] + 0.3 * combined[\"sentiment_score\"]\n",
        "        combined.sort_values(\"composite_score\", ascending=False, inplace=True)\n",
        "        combined.reset_index(drop=True, inplace=True)\n",
        "        return combined\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configure the analysis window\n",
        "\n",
        "Edit the parameters below to control the evaluation period and the news dump location.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "START_DATE = \"2023-01-31\"\n",
        "END_DATE = \"2025-06-30\"\n",
        "NEWS_PATH = Path(\"i need news/news_dumps.json\")\n",
        "\n",
        "forecaster = SectorForecaster(\n",
        "    DEFAULT_LEADERS,\n",
        "    start=START_DATE,\n",
        "    end=END_DATE,\n",
        "    news_path=NEWS_PATH,\n",
        ")\n",
        "forecaster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analyse news sentiment\n",
        "\n",
        "This cell loads the news dump, computes per-sector sentiment, and shows the aggregated scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sentiment_df = forecaster.analyse_news()\n",
        "sentiment_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fetch market data and compute sector growth\n",
        "\n",
        "Yahoo Finance data is downloaded for the defined leaders. The resulting market-cap history and summary table are returned.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "market_history, growth_df = forecaster.fetch_market_trends()\n",
        "growth_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Rank sectors with combined signals\n",
        "\n",
        "The composite score blends normalised growth and sentiment values (70% / 30% weighting).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "ranking_df = forecaster.rank_sectors(sentiment_df, growth_df)\n",
        "ranking_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Optional: Export results\n",
        "\n",
        "Uncomment the lines below to persist CSV outputs in your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# output_dir = Path(\"outputs\")\n",
        "# output_dir.mkdir(parents=True, exist_ok=True)\n",
        "# market_history.to_csv(output_dir / \"market_history.csv\", index=False)\n",
        "# sentiment_df.to_csv(output_dir / \"news_sentiment.csv\", index=False)\n",
        "# ranking_df.to_csv(output_dir / \"sector_rankings.csv\", index=False)\n",
        "# output_dir.resolve()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}